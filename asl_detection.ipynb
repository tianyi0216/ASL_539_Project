{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training, validation and testing samples\n",
      "1512\n",
      "144\n",
      "72\n",
      "min and max width and height of training images:  512 512 512 512\n",
      "min and max width and height of validation images:  512 512 512 512\n",
      "min and max width and height of testing images:  512 512 512 512\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "\n",
    "train_annotation = pd.read_csv('data/train/_annotations.csv')\n",
    "val_annotation = pd.read_csv('data/valid/_annotations.csv')\n",
    "test_annotation = pd.read_csv('data/test/_annotations.csv')\n",
    "\n",
    "# print the number of samples of the data\n",
    "print(\"Number of training, validation and testing samples\")\n",
    "print(len(train_annotation))\n",
    "print(len(val_annotation))\n",
    "print(len(test_annotation))\n",
    "\n",
    "# check the dimension of image is the same\n",
    "print(\"min and max width and height of training images: \", train_annotation['width'].min(), train_annotation['width'].max(), train_annotation['height'].min(), train_annotation['height'].max())\n",
    "print(\"min and max width and height of validation images: \", val_annotation['width'].min(), val_annotation['width'].max(), val_annotation['height'].min(), val_annotation['height'].max())\n",
    "print(\"min and max width and height of testing images: \", test_annotation['width'].min(), test_annotation['width'].max(), test_annotation['height'].min(), test_annotation['height'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function map the class name to an integer\n",
    "def charToVal(c):\n",
    "    return ord(c) - 64\n",
    "\n",
    "def valToChar(v):\n",
    "    return chr(v + 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimension\n",
    "img_width = 512\n",
    "img_height = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset class\n",
    "\n",
    "class ASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom ASL dataset needed for training\n",
    "    \"\"\"\n",
    "    def __init__(self, annotation, img_dir, transforms=None):\n",
    "        # initialize the annotation, paths to images, and transforms\n",
    "        self.annotation = annotation\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the length of the dataset\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.annotation.at[idx, 'filename']\n",
    "        \n",
    "        class_name = self.annotation.at[idx, 'class']\n",
    "        xmin = self.annotation.at[idx, 'xmin']\n",
    "        xmax = self.annotation.at[idx, 'xmax']\n",
    "        ymin = self.annotation.at[idx, 'ymin']\n",
    "        ymax = self.annotation.at[idx, 'ymax']\n",
    "\n",
    "        # read in the image\n",
    "        path = os.path.join(self.img_dir, filename)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # target\n",
    "        target = {}\n",
    "        target['boxes'] = torch.as_tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)\n",
    "        target['labels'] = torch.as_tensor([charToVal(class_name)], dtype=torch.int64)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# create the dataset\n",
    "train_dataset = ASLDataset(train_annotation, 'data/train', data_transform)\n",
    "val_dataset = ASLDataset(val_annotation, 'data/valid', data_transform)\n",
    "test_dataset = ASLDataset(test_annotation, 'data/test', data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_iou(pred_boxes, target_box):\n",
    "    best_iou = 0\n",
    "    best_index = -1\n",
    "    for i, box in enumerate(pred_boxes):\n",
    "        curr_iou = box_iou(target_box, box)\n",
    "        if curr_iou >= best_iou:\n",
    "            best_iou = curr_iou\n",
    "            best_index = i\n",
    "    return best_iou, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = list(images)\n",
    "    targets = list(targets)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloaders\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0.1725, 0.1725, 0.1725,  ..., 0.1647, 0.1647, 0.1647],\n",
      "         [0.1725, 0.1725, 0.1725,  ..., 0.1647, 0.1686, 0.1686],\n",
      "         [0.1765, 0.1765, 0.1765,  ..., 0.1725, 0.1725, 0.1725],\n",
      "         ...,\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.7961, 0.7961, 0.7961],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.8078, 0.8039, 0.8000],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.8196, 0.8118, 0.8039]],\n",
      "\n",
      "        [[0.1725, 0.1725, 0.1725,  ..., 0.1686, 0.1686, 0.1686],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.1686, 0.1686, 0.1686],\n",
      "         [0.1608, 0.1608, 0.1608,  ..., 0.1686, 0.1686, 0.1686],\n",
      "         ...,\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.5529, 0.5529, 0.5529],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.5647, 0.5608, 0.5569],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.5765, 0.5686, 0.5608]],\n",
      "\n",
      "        [[0.1647, 0.1647, 0.1647,  ..., 0.1843, 0.1843, 0.1765],\n",
      "         [0.1608, 0.1608, 0.1608,  ..., 0.1765, 0.1765, 0.1765],\n",
      "         [0.1569, 0.1569, 0.1569,  ..., 0.1608, 0.1608, 0.1608],\n",
      "         ...,\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.2588, 0.2588, 0.2588],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.2706, 0.2667, 0.2627],\n",
      "         [0.1686, 0.1686, 0.1686,  ..., 0.2824, 0.2745, 0.2667]]]), tensor([[[0.8549, 0.8549, 0.8549,  ..., 0.9608, 0.9059, 0.8667],\n",
      "         [0.8549, 0.8549, 0.8549,  ..., 0.8902, 0.8549, 0.8314],\n",
      "         [0.8549, 0.8549, 0.8549,  ..., 0.8196, 0.8078, 0.8000],\n",
      "         ...,\n",
      "         [0.9333, 0.9294, 0.9294,  ..., 0.6902, 0.6902, 0.6902],\n",
      "         [0.9294, 0.9294, 0.9333,  ..., 0.6824, 0.6824, 0.6824],\n",
      "         [0.9294, 0.9333, 0.9373,  ..., 0.6824, 0.6824, 0.6824]],\n",
      "\n",
      "        [[0.7725, 0.7725, 0.7725,  ..., 0.8510, 0.7961, 0.7569],\n",
      "         [0.7725, 0.7725, 0.7725,  ..., 0.7804, 0.7451, 0.7216],\n",
      "         [0.7725, 0.7725, 0.7725,  ..., 0.7098, 0.6980, 0.6902],\n",
      "         ...,\n",
      "         [0.7216, 0.7176, 0.7176,  ..., 0.5294, 0.5294, 0.5294],\n",
      "         [0.7176, 0.7176, 0.7216,  ..., 0.5216, 0.5216, 0.5216],\n",
      "         [0.7176, 0.7216, 0.7255,  ..., 0.5216, 0.5216, 0.5216]],\n",
      "\n",
      "        [[0.6902, 0.6902, 0.6902,  ..., 0.6941, 0.6392, 0.6000],\n",
      "         [0.6902, 0.6902, 0.6902,  ..., 0.6235, 0.5882, 0.5647],\n",
      "         [0.6902, 0.6902, 0.6902,  ..., 0.5569, 0.5451, 0.5373],\n",
      "         ...,\n",
      "         [0.4784, 0.4745, 0.4745,  ..., 0.3098, 0.3098, 0.3098],\n",
      "         [0.4745, 0.4745, 0.4784,  ..., 0.3098, 0.3098, 0.3098],\n",
      "         [0.4745, 0.4784, 0.4824,  ..., 0.3098, 0.3098, 0.3098]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0.0353, 0.0353, 0.0392,  ..., 0.0510, 0.0353, 0.0471],\n",
      "         [0.0627, 0.0627, 0.0627,  ..., 0.0510, 0.0353, 0.0471],\n",
      "         [0.0549, 0.0549, 0.0549,  ..., 0.0510, 0.0353, 0.0471],\n",
      "         ...,\n",
      "         [0.7137, 0.7137, 0.7137,  ..., 0.0353, 0.0275, 0.0275],\n",
      "         [0.7020, 0.7020, 0.6980,  ..., 0.0353, 0.0275, 0.0275],\n",
      "         [0.6510, 0.6471, 0.6431,  ..., 0.0353, 0.0275, 0.0275]],\n",
      "\n",
      "        [[0.0275, 0.0275, 0.0314,  ..., 0.0471, 0.0353, 0.0471],\n",
      "         [0.0471, 0.0471, 0.0471,  ..., 0.0471, 0.0353, 0.0471],\n",
      "         [0.0353, 0.0353, 0.0353,  ..., 0.0471, 0.0353, 0.0471],\n",
      "         ...,\n",
      "         [0.6431, 0.6431, 0.6431,  ..., 0.0431, 0.0431, 0.0431],\n",
      "         [0.6314, 0.6314, 0.6275,  ..., 0.0431, 0.0431, 0.0431],\n",
      "         [0.5804, 0.5765, 0.5725,  ..., 0.0431, 0.0431, 0.0431]],\n",
      "\n",
      "        [[0.0314, 0.0314, 0.0353,  ..., 0.0392, 0.0353, 0.0471],\n",
      "         [0.0431, 0.0431, 0.0431,  ..., 0.0392, 0.0353, 0.0471],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0392, 0.0353, 0.0471],\n",
      "         ...,\n",
      "         [0.5882, 0.5882, 0.5882,  ..., 0.0392, 0.0392, 0.0392],\n",
      "         [0.5765, 0.5765, 0.5725,  ..., 0.0392, 0.0392, 0.0392],\n",
      "         [0.5255, 0.5216, 0.5176,  ..., 0.0392, 0.0392, 0.0392]]])]\n",
      "[{'boxes': tensor([[147., 162., 510., 512.]]), 'labels': tensor([6])}, {'boxes': tensor([[161., 189., 293., 388.]]), 'labels': tensor([21])}, {'boxes': tensor([[ 90., 148., 505., 502.]]), 'labels': tensor([20])}, {'boxes': tensor([[ 82., 212., 318., 374.]]), 'labels': tensor([7])}]\n"
     ]
    }
   ],
   "source": [
    "# check the dataloader\n",
    "images, targets = next(iter(train_dataloader))\n",
    "print(images)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 27\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training preparation\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "H = {}\n",
    "H['train_loss'] = []\n",
    "H['mean_ious'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 87/378 [02:53<09:44,  2.01s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, targets in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        images = list(image.to(device) for image in images)\n",
    "        \n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        #print(len(images))\n",
    "        #print(len(targets))\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += losses.item()\n",
    "    print(f\"Epoch {epoch + 1} Loss: {train_loss/len(train_dataloader)}\")\n",
    "    H[\"training_loss\"].append(train_loss/len(train_dataloader))\n",
    "\n",
    "    model.eval()\n",
    "    mean_ious = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_dataloader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            ious = []\n",
    "            iou_idxs = []\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                target_box = targets[i]['boxes'][0].cpu().numpy()\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "\n",
    "                best_iou, best_idx = find_best_iou(pred_boxes, target_box)\n",
    "                ious.append(best_iou)\n",
    "                iou_idxs.append(best_idx)\n",
    "            mean_ious += sum(ious)/len(ious)\n",
    "    print(f\"Epoch {epoch + 1} Mean IOU: {mean_ious/len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
